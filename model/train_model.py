from model.cnn_model import ChessEvalCNN
from utils.fen_to_tensor import fen_to_tensor
import os
import torch 
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np 
from tqdm import tqdm 


""" Training Script: This script will connect all the peices of the project
"""

# Dataset Class - This wraps our chess data inot a Pytorch dataset 
class ChessDataset(Dataset):
    """
    Custom dataset that: 
        - Reads from the CSV file generated by process_data.py
        - converts each FEN into a tensor (using our fen_to_tensor)
        - provides the target stockfish evaluation as a label
    """

    def __init__(self, csv_path):
        # load the csv inot a pandas DataFrame
        self.data = pd.read_csv(csv_path)
    
    def __len__(self):
        # Required method: returns the total number of samples (pytorch uses this to know how many training examples exist)
        return len(self.data)
    
    def __getitem__(self, idx):
        # Required method: returns one training example at the given index (called when you do dataset[idx])

        # First retrieve the row from the DataFrame 
        row = self.data.iloc[idx]

        # Extract the FEN string and evaluation score
        fen = row ["fen"]
        eval_score = row["eval"]

        # Convert FEN string to tensor rerpresentation
        input_tensor = fen_to_tensor(fen)
        # Returns array of shape (12, 8, 8)

        # Now convert the array to pytorch tensor (note that float() ensures its torch.float32 which is required for neural networks)
        input_tensor = torch.from_numpy(input_tensor).float()

        # Normalize the evaluation this helps training stability 
        normalized_eval = eval_score / 10.0

        # Prepare the label (target value). This is what we want the network to predict
        # we wrap it in a list [eval_score] so it has shape (1,) not ()
        label = torch.tensor([normalized_eval], dtype=torch.float32)

        return input_tensor, label
    


# Training Function 
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """ Train the model for one complete pass through the dataset (one epoch).
    
        Parameters: 
                    - model: The neural network we're training (ChessEvalCNN)
                    - dataloader: Provides batches of (input, label) pairs (DataLoader)
                    - criterion: Loss function that measures how wrong our predictions are 
                    - optimizer: Algoirthm that adjusts weights to reduce loss (optim.Adam)
                    - device: either the CPU or GPU this is where the computations happen 

        Returns: float which is the avg loss across all abtches in this epoch
                    

        Training Loop: For each batch of data there will be a forward pass, where the model makes predictions
                       then we will compute loss to figure out how wrong the model was. After there will be a 
                       backward pass where we calcualte gradients (which direction to adjust weights) and then we will 
                       update the weights based on the gradient. 

    """

    # put the model in training mode
    model.train()

    running_loss = 0.0
    num_batches = 0

    # Loop through batches and use tqdm to see the training progress 
    for inputs, labels in tqdm(dataloader, desc="Training", leave=False):
        """ inputs: Tensor fo shape (batch_size, 12, 8, 8) - chess positions
            labels: Tensor of shape (batch_size, 1) which are stockfish evals
        """

        # 1. Move data to correct device (GPU or CPU)
        inputs = inputs.to(device)
        labels = labels.to(device)
        # Note that Neural networcks run much faster on gpus and if the device is gpu, this copies the data
        # from RAM to GPU mem

        # 2. Zero the gradients. We do this since pytorch accumulates gradients by defualt so if we don't reset them,
        #    gradients from this batch would add to gradients from the previous patch 
        optimizer.zero_grad()

        # 3. Forward pass note that this calls model.forward(inputs) in cnn_model.py
        outputs = model(inputs)

        # So at this point the input has flowed through all the layers of the CNN and each position got one predicted evaluation so we can go ahead and compute the loss

        # 4. Compute loss
        loss = criterion(outputs, labels)

        # 5. Backward pass. We use .backward() whihc uses backpropagation to compute how much each of the weights contributed to the loss. 
        #    In short for each weight this calculates: gradient = deltalosss/deltaweight which tells us how the loss changes with respect to the weight. 
        #    This uses the chain rule to trace the error signal backwards through all the layers so fc3, fc2, fc1, conv3, conv2, conv1 and after this each weight has a .grad 
        #    attribute containing its gradient 
        loss.backward()

        # 6. Update the weights. The optimizer will now update each weight based on its gradient using: 
        #    new_weight = old_weight - (learning_rate * gradient). A negative gradient -> the loss decreases as the weight increases so the new weight would be lower and vise versa for positive gradient. 
        optimizer.step()

        # Track the loss for this batch 
        running_loss += loss.item()
        num_batches += 1

    # Calculate the avg loss across all batches 
    avg_loss = running_loss / num_batches 

    return avg_loss



# Function to test how well the model works
def validate(model, dataloader, criterion, device): 
    """ This function will basically test how good the model is on positions it has never seen during training. 
        Hence we can check how well training went and if the model actually learned how to evaluate chess positions or just memorized the training data.
    """

    # Put the model in evaluation mode (note that in training mdoe we pote the model in training mode)
    model.eval()
    running_loss = 0.0
    num_batches = 0

    # we're not gonna need gradients for validation (no backward passes and no updating weights purely testing). This will also save mem and speed thigns up
    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc="Validating", leave=False):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Forward pass only 
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            num_batches += 1

    avg_loss = running_loss / num_batches 

    return avg_loss


# Main function that orchestrates the entire process
def main(): 
    
    # Configuration 
    CSV_PATH = "data/chess_positions.csv"
    BATCH_SIZE = 32     # How many positions to process at once 
    LEARNING_RATE = 0.0001   # How big of steps to take during optimization 
    NUM_EPOCHS = 30     # How many times to go through the entire dataset
    TRAIN_SPLIT = 0.8    # We will be using 80-20 split so 80% training and 20% validation


    # 1. Load the data set 
    dataset = ChessDataset(CSV_PATH)

    # 2. Split into training and validation sets. 
    train_size = int(TRAIN_SPLIT * len(dataset)) 
    val_size = len(dataset) - train_size 

    # Model will learn from the training set and will be evaluated on the validation set. This prevents OVERFITTING which is when the model memorizes training 
    # data but cant generalize to new positions so we will use a portion of the data for training and another portion for validation. 
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))

    # 3. Create DataLoaders. Notes for self: DataLoader is a pytorch utility that: 1. Groups samples into batches (e.g., 32 positiosn at a time),
    #                                        2. Shuffles the data each epoch so the model sees different combinations. 3. loads data in parallel (faster training)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

    # Note that there's no need to shuffle validation data so we set shuffle to false
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)


    # 4. Set up device (GPU or CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n using device: {device}")

    # 5. Initialize model, loss function and optimizer 
    model = ChessEvalCNN().to(device)
    # Note that .to(device) will move the model to the gpu if its available and if it is it would move all the weights from ram to gpu mem

    # We will use Mean squared error whihc is standard loss for regression.
    criterion = nn.MSELoss()
    
    # We will use Adam for our optimizer (good default choice based on the internet!)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    

    print(f"\n Starting training for {NUM_EPOCHS} epochs...")
    print("=" * 70)
    best_val_loss = float('inf') # use to track the best validation loss

    # 6. Training loop 
    for epoch in range(NUM_EPOCHS): 
        # Train for one epoch 
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss = validate(model, val_loader, criterion, device)

        # Print the results 
        print(f"   Training Loss:   {train_loss:.6f}")
        print(f"   Validation Loss: {val_loss:.6f}")


        # Save the best model (lowest validation loss) 
        if val_loss < best_val_loss: 
            best_val_loss = val_loss
            os.makedirs("model", exist_ok=True)
            torch.save(model.state_dict(), "model/chess_eval_cnn_best.pth")

    
    print("\n" + "=" * 70)
    print(" TRAINING COMPLETE!")
    print("=" * 70)

    # 7. Save the final mode
    os.makedirs("model", exist_ok=True)
    torch.save(model.state_dict(), "model/chess_eval_cnn_final.pth")



# Script entry point 
if __name__ == "__main__":
    main()

    

